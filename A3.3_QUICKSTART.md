# A3.3 Store Ingestion Pipeline - Quick Start

**5-Minute Guide to Using the Store Ingestion Pipeline**

## What is This?

The store ingestion pipeline automatically fetches WIC store data from state sources, validates it, and saves it to the database. It runs monthly via cron or can be triggered manually.

## Quick Commands

```bash
# Test the pipeline (no database writes)
npm run ingest-stores -- --dry-run

# Ingest all states
npm run ingest-stores

# Ingest specific states
npm run ingest-stores -- --states=MI,NC

# Check pipeline health
npm run store-health-check

# Run examples
npm run test-store-ingestion

# Get help
npm run ingest-stores -- --help
```

## How It Works

```
State Websites â†’ Scrapers (A3.1) â†’ Pipeline (A3.3) â†’ Database (A3.2)
```

**Pipeline Steps:**
1. **Scrape** - Fetch raw data from state sources
2. **Normalize** - Convert to standard format
3. **Validate** - Check data quality
4. **Store** - Save to database (insert new, update existing)
5. **Monitor** - Track health and metrics

## Files You Need to Know

| File | Purpose | When to Use |
|------|---------|-------------|
| `src/cli/ingest-stores.ts` | Manual ingestion | Run on-demand |
| `src/jobs/store-ingestion.job.ts` | Scheduled job | Cron automation |
| `src/config/ingestion-schedule.config.ts` | Schedule config | Change frequency |
| `src/examples/store-ingestion-example.ts` | Usage examples | Learn API |
| `src/services/store/README.md` | Full docs | Deep dive |

## Common Tasks

### 1. Test the Pipeline

```bash
# Dry run - see what would happen without DB writes
npm run ingest-stores -- --dry-run
```

**Expected output:**
```
Scraping MI... âœ“ (50 stores)
Scraping NC... âœ“ (60 stores)
Scraping FL... âœ“ (60 stores)
Scraping OR... âœ“ (50 stores)

DRY RUN: Would insert 220 stores
```

### 2. Ingest One State

```bash
# Test with just Michigan
npm run ingest-stores -- --states=MI
```

**Expected output:**
```
Michigan:
  Scraped:    50
  Normalized: 50
  Inserted:   45
  Updated:    5
  Duration:   10.2s
```

### 3. Check Pipeline Health

```bash
npm run store-health-check
```

**Expected output:**
```
Health Status: âœ… Healthy
Last Successful Run: 2026-01-15T02:00:00Z
Days Since Last Run: 6
Recent Failures: 0
```

### 4. Schedule Automated Runs

**Add to crontab:**
```bash
# Run monthly on 1st at 2 AM
0 2 1 * * cd /path/to/project && node src/jobs/store-ingestion.job.ts --notify
```

**Or use the config:**
```typescript
// src/config/ingestion-schedule.config.ts
export const DEFAULT_SCHEDULE = {
  cronExpression: '0 2 1 * *', // Monthly
  states: ['MI', 'NC', 'FL', 'OR'],
  notifyOnFailure: true,
};
```

## Data Quality

The pipeline validates stores and assigns quality scores (0-100):

| Score | Quality | Meaning |
|-------|---------|---------|
| 90-100 | Excellent | All fields complete and valid |
| 80-89 | Good | Minor missing optional fields |
| 70-79 | Fair | Some required fields missing |
| <70 | Poor | Major data issues, may skip |

**Quality Checks:**
- âœ… Required fields (name, address, coordinates)
- âœ… Valid formats (ZIP, phone, lat/lng)
- âœ… WIC authorization status
- âš ï¸ Optional fields (hours, phone, features)

## Monitoring

**Health Alerts:**
- ðŸ”´ **CRITICAL**: No success in >35 days
- ðŸŸ¡ **WARNING**: No success in >30 days
- ðŸŸ¡ **WARNING**: High failure rate (>5 in 30 days)

**Logs:**
- `logs/ingestion/*.json` - Detailed ingestion logs
- Console output - Real-time progress

## Error Handling

**Pipeline Behavior:**
- âœ… Continues on scraper failures
- âœ… Skips invalid stores, logs errors
- âœ… Retries on next scheduled run
- âœ… Reports all errors at end

**Recovery:**
```bash
# Retry failed state
npm run ingest-stores -- --states=MI

# Check logs for details
cat logs/ingestion/*.json | grep -A5 "error"
```

## Integration

**Uses:**
- **A3.1** - `RetailerDataService` for scraping
- **A3.2** - `StoreRepository` for database

**Provides Data For:**
- **A3.4** - Google Places enrichment (hours, photos)
- **A3.5** - Store search API (user queries)

## Troubleshooting

### Pipeline won't start

```bash
# Check dependencies
npm install

# Verify database connection
psql -d wic_benefits -c "SELECT 1;"

# Test with dry run
npm run ingest-stores -- --dry-run
```

### No data ingested

```bash
# Check scrapers
cd src/services/retailer
npm run test

# Verify data source
npm run ingest-stores -- --states=MI --dry-run
```

### Low quality scores

```bash
# Run validation report
npm run test-store-ingestion

# Check specific issues
# Review console output for warnings
```

### Stale data alert

```bash
# Run manual ingestion
npm run ingest-stores

# Check cron job
crontab -l | grep store-ingestion

# Review recent failures
npm run store-health-check
```

## Examples

**See:** `src/examples/store-ingestion-example.ts`

```typescript
import { StoreIngestionPipeline } from './services/store';

// Example 1: Basic ingestion
const pipeline = new StoreIngestionPipeline();
const result = await pipeline.ingest();

// Example 2: Specific states
const result = await pipeline.ingest({
  states: ['MI', 'NC'],
});

// Example 3: Dry run
const result = await pipeline.ingest({
  dryRun: true,
});
```

## Next Steps

1. **Test**: `npm run ingest-stores -- --dry-run`
2. **Ingest**: `npm run ingest-stores -- --states=MI`
3. **Verify**: Check database for new stores
4. **Schedule**: Set up cron job for automated runs
5. **Monitor**: Check health regularly

## Get Help

- **Full docs**: `src/services/store/README.md`
- **Examples**: `src/examples/store-ingestion-example.ts`
- **CLI help**: `npm run ingest-stores -- --help`
- **API reference**: TypeScript interfaces in each file

---

**Quick Reference Card**

| Command | Purpose |
|---------|---------|
| `npm run ingest-stores -- --dry-run` | Test pipeline |
| `npm run ingest-stores` | Ingest all states |
| `npm run ingest-stores -- --states=MI` | Ingest one state |
| `npm run store-health-check` | Check status |
| `npm run test-store-ingestion` | Run examples |

**Default Schedule**: Monthly on 1st at 2 AM ET
**States**: MI, NC, FL, OR
**Batch Size**: 50 stores
**Duration**: ~45 seconds (mock data)
